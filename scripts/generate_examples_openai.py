#!/usr/bin/env python3
"""
æ‰¹é‡ç”Ÿæˆä¾‹å¥è„šæœ¬ï¼ˆOpenAI GPT-4o-miniï¼‰
ä¸ºæ²¡æœ‰ä¾‹å¥çš„è¯æ¡ç”Ÿæˆä¾‹å¥å¹¶å­˜å…¥æ•°æ®åº“
"""

import sqlite3
import os
import sys
import time
import json
from typing import List, Tuple, Dict
from openai import OpenAI

# OpenAI API é…ç½®
API_KEY = os.environ.get('OPENAI_API_KEY')
if not API_KEY:
    print("âŒ é”™è¯¯: è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    sys.exit(1)
MODEL_NAME = "gpt-4o-mini"

# æ•°æ®åº“è·¯å¾„
DB_PATHS = [
    "../NichiDict/Resources/seed.sqlite"
]

# æ‰¹é‡å¤„ç†å‚æ•°
BATCH_SIZE = 50  # æ¯æ‰¹å¤„ç†çš„è¯æ¡æ•°
EXAMPLES_PER_WORD = 3  # æ¯ä¸ªè¯ç”Ÿæˆ3ä¸ªä¾‹å¥
DELAY_BETWEEN_BATCHES = 0.5  # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆç§’ï¼‰
TOP_N_WORDS = 5000  # å¤„ç†å‰5000ä¸ªè¯

def init_openai():
    """åˆå§‹åŒ– OpenAI API"""
    client = OpenAI(api_key=API_KEY)
    return client

def get_words_without_examples(db_path: str, top_n: int) -> List[Tuple]:
    """
    è·å–æ²¡æœ‰ä¾‹å¥çš„è¯æ¡ï¼ˆå‰Nä¸ªï¼‰
    è¿”å›: [(entry_id, headword, reading_hiragana, reading_romaji, sense_id, definition_english, definition_chinese), ...]
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # è·å–å‰Nä¸ªè¯æ¡ä¸­æ²¡æœ‰ä¾‹å¥çš„
    cursor.execute(f"""
        SELECT DISTINCT
            d.id as entry_id,
            d.headword,
            d.reading_hiragana,
            d.reading_romaji,
            s.id as sense_id,
            s.definition_english,
            COALESCE(s.definition_chinese_simplified, s.definition_chinese_traditional, '') as definition_chinese
        FROM dictionary_entries d
        JOIN word_senses s ON d.id = s.entry_id
        WHERE d.id <= {top_n}
          AND s.id NOT IN (SELECT DISTINCT sense_id FROM example_sentences)
        ORDER BY d.id
    """)

    words = cursor.fetchall()
    conn.close()

    return words

def generate_examples_for_word(client, word: Tuple) -> Tuple[int, List[Dict]]:
    """
    ä¸ºä¸€ä¸ªè¯ç”Ÿæˆä¾‹å¥
    è¿”å›: (sense_id, [example1, example2, example3])
    """
    entry_id, headword, reading_hiragana, reading_romaji, sense_id, def_en, def_cn = word

    prompt = f"""Generate {EXAMPLES_PER_WORD} natural Japanese example sentences for this word.

Word: {headword}
Reading: {reading_hiragana} ({reading_romaji})
Meaning: {def_en}
{f'ä¸­æ–‡: {def_cn}' if def_cn else ''}

Requirements:
1. Generate {EXAMPLES_PER_WORD} natural Japanese sentences (20-30 characters each)
2. Each sentence must demonstrate typical usage in daily life
3. Keep sentences simple and practical
4. Include the word '{headword}' or its conjugated form

Return ONLY a JSON object with this schema:
{{"examples":[
  {{"japanese":"...", "chinese":"...", "english":"..."}},
  {{"japanese":"...", "chinese":"...", "english":"..."}},
  {{"japanese":"...", "chinese":"...", "english":"..."}}
]}}

Respond with JSON only."""

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are a Japanese language expert. Generate natural example sentences."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500
        )

        response_text = response.choices[0].message.content.strip()

        # æ¸…ç† markdown ä»£ç å—æ ‡è®°
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        response_text = response_text.strip()

        # è§£æ JSON
        data = json.loads(response_text)
        examples = data.get("examples", [])

        return (sense_id, examples)

    except Exception as e:
        print(f"    âŒ {headword} ç”Ÿæˆå¤±è´¥: {str(e)[:100]}")
        return (sense_id, [])

def generate_examples_for_batch(client, words: List[Tuple]) -> Dict[int, List[Dict]]:
    """
    ä¸ºä¸€æ‰¹è¯ç”Ÿæˆä¾‹å¥
    è¿”å›: {sense_id: [example1, example2, example3], ...}
    """
    if not words:
        return {}

    results = {}

    for word in words:
        _, headword, reading_hiragana, _, sense_id, _, _ = word

        sense_id, examples = generate_examples_for_word(client, word)

        if examples:
            results[sense_id] = examples
            print(f"    âœ… {headword} ({reading_hiragana}): {len(examples)} ä¾‹å¥")
        else:
            print(f"    âš ï¸  {headword}: ç”Ÿæˆå¤±è´¥")

    return results

def insert_examples(db_path: str, examples_by_sense: Dict[int, List[Dict]]):
    """
    å°†ç”Ÿæˆçš„ä¾‹å¥æ’å…¥æ•°æ®åº“
    """
    if not examples_by_sense:
        return 0

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    total_inserted = 0

    try:
        for sense_id, examples in examples_by_sense.items():
            for idx, example in enumerate(examples, 1):
                japanese = example.get("japanese", "")
                chinese = example.get("chinese", "")
                english = example.get("english", "")

                if not japanese or not english:
                    continue

                cursor.execute("""
                    INSERT INTO example_sentences
                    (sense_id, japanese_text, english_translation, chinese_translation, example_order)
                    VALUES (?, ?, ?, ?, ?)
                """, (sense_id, japanese, english, chinese if chinese else None, idx))
                total_inserted += 1

        conn.commit()

    except Exception as e:
        conn.rollback()
        print(f"    âŒ æ•°æ®åº“æ’å…¥å¤±è´¥: {e}")
    finally:
        conn.close()

    return total_inserted

def process_database(db_path: str):
    """
    å¤„ç†ä¸€ä¸ªæ•°æ®åº“
    """
    if not os.path.exists(db_path):
        print(f"âš ï¸  æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
        return

    print(f"\n{'='*60}")
    print(f"å¼€å§‹å¤„ç†æ•°æ®åº“: {db_path}")
    print(f"{'='*60}")

    # è·å–éœ€è¦ç”Ÿæˆä¾‹å¥çš„è¯
    words = get_words_without_examples(db_path, TOP_N_WORDS)
    total = len(words)

    if total == 0:
        print("âœ… æ‰€æœ‰è¯æ¡éƒ½å·²æœ‰ä¾‹å¥")
        return

    print(f"ğŸ“Š éœ€è¦ç”Ÿæˆä¾‹å¥çš„è¯æ¡: {total} ä¸ª")
    print(f"ğŸ“Š é¢„è®¡ç”Ÿæˆä¾‹å¥: {total * EXAMPLES_PER_WORD} æ¡")
    print(f"ğŸ“Š é¢„è®¡æ‰¹æ¬¡: {(total + BATCH_SIZE - 1) // BATCH_SIZE} æ‰¹")

    # ä¼°ç®—æˆæœ¬
    input_tokens = total * 200
    output_tokens = total * 400
    cost = (input_tokens / 1_000_000 * 0.150) + (output_tokens / 1_000_000 * 0.600)
    print(f"ğŸ’° é¢„è®¡æˆæœ¬: ${cost:.2f} USD")
    print(f"â±ï¸  é¢„è®¡æ—¶é—´: {total / 60:.1f} åˆ†é’Ÿ")

    # ç¡®è®¤
    print(f"\nâš ï¸  å‡†å¤‡å¼€å§‹ç”Ÿæˆï¼Œå°†æ¶ˆè€— API é…é¢")
    print(f"æŒ‰ Ctrl+C å–æ¶ˆï¼Œæˆ–ç­‰å¾… 5 ç§’è‡ªåŠ¨å¼€å§‹...")
    time.sleep(5)

    # åˆå§‹åŒ– OpenAI
    client = init_openai()

    # æ‰¹é‡å¤„ç†
    processed_count = 0
    total_examples_inserted = 0
    batch_num = 0

    for i in range(0, total, BATCH_SIZE):
        batch = words[i:i + BATCH_SIZE]
        batch_num += 1

        print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num}/{(total + BATCH_SIZE - 1) // BATCH_SIZE} "
              f"(è¯æ¡ {i+1}-{min(i+BATCH_SIZE, total)}/{total})")

        # ç”Ÿæˆä¾‹å¥
        examples_by_sense = generate_examples_for_batch(client, batch)

        # æ’å…¥æ•°æ®åº“
        if examples_by_sense:
            inserted = insert_examples(db_path, examples_by_sense)
            total_examples_inserted += inserted
            processed_count += len(examples_by_sense)
            print(f"    ğŸ’¾ æ’å…¥ {inserted} æ¡ä¾‹å¥")

        # æ˜¾ç¤ºè¿›åº¦
        progress = (i + len(batch)) / total * 100
        print(f"ğŸ“ˆ è¿›åº¦: {progress:.1f}% ({processed_count}/{total} è¯ï¼Œ{total_examples_inserted} ä¾‹å¥)")

        # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        if i + BATCH_SIZE < total:
            time.sleep(DELAY_BETWEEN_BATCHES)

    print(f"\nâœ… æ•°æ®åº“å¤„ç†å®Œæˆï¼")
    print(f"   å¤„ç†è¯æ•°: {processed_count}")
    print(f"   ç”Ÿæˆä¾‹å¥: {total_examples_inserted} æ¡")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print(f"ğŸ“š ä¾‹å¥æ‰¹é‡ç”Ÿæˆè„šæœ¬ (OpenAI {MODEL_NAME})")
    print("=" * 60)
    print(f"ç›®æ ‡èŒƒå›´: å‰ {TOP_N_WORDS} ä¸ªè¯æ¡ï¼ˆæ— ä¾‹å¥ï¼‰")
    print(f"æ¯è¯ä¾‹å¥: {EXAMPLES_PER_WORD} æ¡")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print()

    # åˆ‡æ¢åˆ°è„šæœ¬ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)

    # å¤„ç†æ‰€æœ‰æ•°æ®åº“
    for db_path in DB_PATHS:
        process_database(db_path)

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æ•°æ®åº“å¤„ç†å®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
